{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "import mrmr\n",
    "import matplotlib.pyplot as plt\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the list of feature names\n",
    "feature_names =  ['age', 'rank', 'height', 'weight', 'bmi', 'bp_s', 'bp_d', 'bp', 'map', 'smoker', 'alcohol', 'faam', 'eilp', 'chronicity', 'ttp', 'ttd', 'ttt', 'wait time', 'time_dg', 'co_morb', 'prior_injuries', 'prior_surgery', 'prior_courses']\n",
    "\n",
    "# Read the feature data from a CSV file into a Pandas DataFrame\n",
    "X = pd.read_csv('data//X.csv', header=None)\n",
    "\n",
    "# Assign the feature names to the columns of the DataFrame\n",
    "X.columns = feature_names\n",
    "\n",
    "# Read the target variable (labels) from a CSV file into a Pandas DataFrame\n",
    "y = pd.read_csv('data//y.csv', header=None)\n",
    "\n",
    "# Assign a column name ('labels') to the target variable\n",
    "y.columns = ['labels']\n",
    "\n",
    "# Use the mrmr_classif function to select features based on minimum redundancy maximum relevance\n",
    "selected_features = mrmr.mrmr_classif(X, y.iloc[:, 0].values, K=X.shape[1])\n",
    "\n",
    "# Initialize a list with the first selected feature\n",
    "valid_features = [selected_features[0]]\n",
    "\n",
    "# Iterate over the remaining selected features and check for correlation with already selected features\n",
    "for feat in range(1, X.shape[1]):\n",
    "    # Check if the absolute correlation between the current feature and the valid features is greater than 0.5\n",
    "    if any(abs(X[valid_features + [selected_features[feat]]].corr().iloc[:-1, -1].values) > 0.5):\n",
    "        # If the correlation is greater than 0.5, skip adding the feature to the valid features list\n",
    "        pass\n",
    "    else:\n",
    "        # If the correlation is not greater than 0.5, add the feature to the valid features list\n",
    "        valid_features.append(selected_features[feat])\n",
    "\n",
    "# Create a new DataFrame (X_) containing only the selected valid features\n",
    "X_ = X[valid_features].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.pipeline import Pipeline\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import cross_validate, StratifiedKFold, RandomizedSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid_logistic = {\n",
    "    'C': [0.001, 0.01, 0.1, 0.5, 1, 10, 100],\n",
    "    'penalty': ['l1', 'l2'],\n",
    "    'solver': ['liblinear', 'saga']\n",
    "}\n",
    "\n",
    "param_grid_svc = {\n",
    "    'C': [0.001, 0.01, 0.1, 0.5, 1, 10, 100],\n",
    "    'kernel': ['linear', 'rbf', 'poly', 'sigmoid'],\n",
    "    'gamma': ['scale', 'auto']\n",
    "}\n",
    "\n",
    "param_grid_rf = {\n",
    "    'n_estimators': [10, 20, 50, 100],\n",
    "    'max_features': ['sqrt', 'log2'],\n",
    "    'max_depth': [1, 2]\n",
    "}\n",
    "\n",
    "param_grid_lda = {}  # LinearDiscriminantAnalysis does not have hyperparameters to tune\n",
    "\n",
    "param_grid_xgb = {\n",
    "    'learning_rate': [0.001, 0.005, 0.01, 0.05, 0.1],\n",
    "    'max_depth': [1, 2],\n",
    "    'n_estimators': [10, 20, 50, 100],\n",
    "    'booster': ['gbtree', 'gblinear', 'dart']\n",
    "}\n",
    "\n",
    "models = [LogisticRegression(class_weight='balanced'),\n",
    "          SVC(probability=True, max_iter=100,class_weight='balanced'),\n",
    "          RandomForestClassifier(class_weight='balanced',warm_start=True),\n",
    "          Pipeline([('smote', SMOTE(sampling_strategy='auto', random_state=42)), ('clf', LinearDiscriminantAnalysis())]),\n",
    "          Pipeline([('smote', SMOTE(sampling_strategy='auto', random_state=42)), ('clf', XGBClassifier())])]\n",
    "\n",
    "model_names = ['Logistic Regression',\n",
    "               'SVM',\n",
    "               'Random Forest',\n",
    "               'Linear Discriminant Analysis',\n",
    "               'XGBoost']\n",
    "\n",
    "# Perform hyperparameter tuning for each model\n",
    "tuned_models = []\n",
    "for model, param_grid in zip(models, [param_grid_logistic, param_grid_svc, param_grid_rf, param_grid_lda, param_grid_xgb]):\n",
    "    if isinstance(model, Pipeline):\n",
    "        # If the model is a pipeline, update the 'clf' step with RandomizedSearchCV\n",
    "        model_name = model.steps[-1][0]\n",
    "        tuned_model = Pipeline([\n",
    "            ('smote', model.steps[0][1]),  # Assuming the first step is 'smote'\n",
    "            ('clf', RandomizedSearchCV(model.steps[-1][1], param_distributions=param_grid, scoring='roc_auc', n_iter=10, random_state=42, cv=4))\n",
    "        ])\n",
    "    else:\n",
    "        # If the model is not a pipeline, wrap it with RandomizedSearchCV\n",
    "        model_name = model.__class__.__name__\n",
    "        tuned_model = RandomizedSearchCV(model, param_distributions=param_grid, scoring='roc_auc', n_iter=10, random_state=42, cv=4)\n",
    "\n",
    "    tuned_models.append(tuned_model)\n",
    "\n",
    "# Create the model dictionary\n",
    "model_dict = dict(zip(model_names, tuned_models))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from interrogation_analysis import ConceptDriftAnalysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noise = np.random.normal(loc=0, scale=0.05, size=X_.values.shape)\n",
    "X_noisy = X_.values + noise\n",
    "\n",
    "drift_analyser = ConceptDriftAnalysis(X_noisy, y['labels'].values, f1_target=1, n1_target=1, pop_size=40, n_gen=10)\n",
    "\n",
    "drift_analyser.X = X_.values\n",
    "drift_analyser.X_ = np.round(drift_analyser.X_,0)\n",
    "\n",
    "drift_results = {}\n",
    "for name, model in model_dict.items():\n",
    "    drift_results[name] = drift_analyser.evaluate_concept_drift(model,n_splits=5,scoring='roc_auc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xticks = []\n",
    "for v in drift_results['Logistic Regression'].values():\n",
    "    if v['std_f1'] == 0:\n",
    "        xticks.append(f\"F1 = {np.round(v['mean_f1'],2)}\\nN1 = {np.round(v['mean_n1'],2)}\")\n",
    "    else:\n",
    "        xticks.append(f\"F1 = {np.round(v['mean_f1'],2)} ± {np.round(v['std_f1'],2)}\\nN1 = {np.round(v['mean_n1'],2)} ± {np.round(v['std_n1'],2)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the desired width and height for the figure\n",
    "fig_width = 12\n",
    "fig_height = 6\n",
    "\n",
    "# Create a new figure with the specified size\n",
    "plt.figure(figsize=(fig_width, fig_height))\n",
    "\n",
    "# Define line styles and markers for each line\n",
    "line_styles = ['--', ':', '-.', '-', '-']\n",
    "markers = ['o', 's', '^', 'D', 'v']\n",
    "\n",
    "# Your existing code with modified line and marker styles\n",
    "for i, (k, v) in enumerate(drift_results.items()):\n",
    "    mean_scores = []\n",
    "    std_scores = []\n",
    "    for k_, v_ in v.items():\n",
    "        mean_scores.append(v_['mean_score'])\n",
    "\n",
    "    plt.plot(mean_scores, linestyle=line_styles[i], marker=markers[i])\n",
    "\n",
    "# Add text boxes under the first and last tick labels\n",
    "plt.text(0.04, -0.15, 'Original\\nDataset', transform=plt.gca().transAxes, ha='center', va='center', fontweight='bold')\n",
    "plt.text(0.23, -0.15, 'Increasingly Complex\\nDatasets', transform=plt.gca().transAxes, ha='center', va='center', fontweight='bold')\n",
    "\n",
    "plt.annotate('', xy=(0.96, -0.15), xycoords='axes fraction', xytext=(0.35, -0.15),\n",
    "arrowprops=dict(arrowstyle=\"->\", color='k'))\n",
    "\n",
    "plt.legend(drift_results.keys())\n",
    "plt.xticks(ticks=[0, 1, 2, 3, 4, 5], labels=xticks)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
