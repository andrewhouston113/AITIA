{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import warnings\n",
    "import mrmr\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the list of feature names\n",
    "feature_names =  ['age', 'rank', 'height', 'weight', 'bmi', 'bp_s', 'bp_d', 'bp', 'map', 'smoker', 'alcohol', 'faam', 'eilp', 'chronicity', 'ttp', 'ttd', 'ttt', 'wait time', 'time_dg', 'co_morb', 'prior_injuries', 'prior_surgery', 'prior_courses']\n",
    "\n",
    "# Read the feature data from a CSV file into a Pandas DataFrame\n",
    "X = pd.read_csv('data//X.csv', header=None)\n",
    "\n",
    "# Assign the feature names to the columns of the DataFrame\n",
    "X.columns = feature_names\n",
    "\n",
    "# Read the target variable (labels) from a CSV file into a Pandas DataFrame\n",
    "y = pd.read_csv('data//y.csv', header=None)\n",
    "\n",
    "# Assign a column name ('labels') to the target variable\n",
    "y.columns = ['labels']\n",
    "\n",
    "# Use the mrmr_classif function to select features based on minimum redundancy maximum relevance\n",
    "selected_features = mrmr.mrmr_classif(X, y.iloc[:, 0].values, K=X.shape[1])\n",
    "\n",
    "# Initialize a list with the first selected feature\n",
    "valid_features = [selected_features[0]]\n",
    "\n",
    "# Iterate over the remaining selected features and check for correlation with already selected features\n",
    "for feat in range(1, X.shape[1]):\n",
    "    # Check if the absolute correlation between the current feature and the valid features is greater than 0.5\n",
    "    if any(abs(X[valid_features + [selected_features[feat]]].corr().iloc[:-1, -1].values) > 0.5):\n",
    "        # If the correlation is greater than 0.5, skip adding the feature to the valid features list\n",
    "        pass\n",
    "    else:\n",
    "        # If the correlation is not greater than 0.5, add the feature to the valid features list\n",
    "        valid_features.append(selected_features[feat])\n",
    "\n",
    "# Create a new DataFrame (X_) containing only the selected valid features\n",
    "X_ = X[valid_features].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis, QuadraticDiscriminantAnalysis\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [LogisticRegression(class_weight='balanced',C=0.25,penalty='l2',solver='liblinear'),\n",
    "          SVC(probability=True, max_iter=100,class_weight='balanced',C=0.45,gamma='scale',kernel='linear'),\n",
    "          Pipeline([('smote', SMOTE(sampling_strategy='auto', random_state=42)), ('clf', KNeighborsClassifier(metric='manhattan',weights='uniform',n_neighbors=11,algorithm='brute'))]),\n",
    "          RandomForestClassifier(class_weight='balanced',max_features='sqrt',n_estimators=20,max_depth=3,warm_start=True),\n",
    "          Pipeline([('smote', SMOTE(sampling_strategy='auto', random_state=42)), ('clf', AdaBoostClassifier())]),\n",
    "          Pipeline([('smote', SMOTE(sampling_strategy='auto', random_state=42)), ('clf', GaussianNB())]),\n",
    "          Pipeline([('smote', SMOTE(sampling_strategy='auto', random_state=42)), ('clf', LinearDiscriminantAnalysis())]),\n",
    "          Pipeline([('smote', SMOTE(sampling_strategy='auto', random_state=42)), ('clf', QuadraticDiscriminantAnalysis())]),\n",
    "          Pipeline([('smote', SMOTE(sampling_strategy='auto', random_state=42)), ('clf', MLPClassifier(max_iter=10))]),\n",
    "          Pipeline([('smote', SMOTE(sampling_strategy='auto', random_state=42)), ('clf', XGBClassifier(learning_rate=0.1,max_depth=3,n_estimators=20,booster='gblinear'))])]\n",
    "\n",
    "model_names = ['Logistic Regression',\n",
    "               'SVM',\n",
    "               'KNN',\n",
    "               'Random Forest',\n",
    "               'Adaboost',\n",
    "               'Naive Bayes',\n",
    "               'Linear Discriminant Analysis',\n",
    "               'Quadratic Discriminant Analysis',\n",
    "               'Neural Network',\n",
    "               'XGBoost']\n",
    "\n",
    "model_dict = dict(zip(model_names, models))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conduct Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from interrogation_analysis import CompetencyAnalysis\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "competency_analyser = CompetencyAnalysis(n_datasets=200, pop_size=30, n_gen=10)\n",
    "competency_analyser.prepare_analysis(X_.values,y['labels'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_models(grid, x_coords, y_coords):\n",
    "    titles = ['Top Ranked Model', '2nd Ranked Model', '3rd Ranked Model']\n",
    "\n",
    "    # Set the size of the figure\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(20, 7))\n",
    "\n",
    "    # Define a custom color map with 10 colors\n",
    "    colors = sns.color_palette(\"pastel\", 10)\n",
    "\n",
    "    # Loop through the top 3 ranked models\n",
    "    for rank in range(3):\n",
    "        # Get indices for the current rank\n",
    "        indices = np.argsort(grid, axis=2)[:, :, -rank - 1]\n",
    "\n",
    "        # Flatten the coordinates and sorted indices\n",
    "        x_flattened = x_coords.flatten()\n",
    "        y_flattened = y_coords.flatten()\n",
    "        indices_flattened = indices.flatten()\n",
    "        c = [colors[i] for i in indices_flattened]\n",
    "\n",
    "        # Create a scatter plot for the current rank using Seaborn\n",
    "        sns.scatterplot(x=x_flattened, y=y_flattened, c=c, marker='o', ax=axes[rank])\n",
    "        axes[rank].scatter(competency_analyser.f1_score, competency_analyser.n1_score, color='red', edgecolors='k', linewidths=1, marker='s', s=50, label='Original Dataset')\n",
    "\n",
    "        # Remove color legend from the Seaborn plot\n",
    "        axes[rank].set_title(titles[rank], fontsize=16)  # Set title fontsize\n",
    "        axes[rank].set_xlabel('F1 Score', fontsize=14)  # Set xlabel fontsize\n",
    "        axes[rank].set_ylabel('N1 Score', fontsize=14)  # Set ylabel fontsize\n",
    "        axes[rank].set_ylim(y_coords[0][0], y_coords[-1][-1])\n",
    "        axes[rank].set_xlim(x_coords[0][0], x_coords[-1][-1])\n",
    "\n",
    "    # Create a legend for the models and original specification\n",
    "    legend_handles = []\n",
    "    legend_labels = []\n",
    "    for i, model in enumerate(model_names):\n",
    "        legend_handles.append(plt.scatter([0], [0], marker='o', c=colors[i], label=model))\n",
    "        legend_labels.append(model)\n",
    "\n",
    "    # Add the legend to the figure\n",
    "    legend_handles.append(plt.scatter([0], [0], marker='s', edgecolors='k', linewidths=1, s=50, color='red', label='Original Dataset'))\n",
    "    legend_labels.append('Original Dataset')\n",
    "    fig.legend(handles=legend_handles, labels=legend_labels, loc='upper right', bbox_to_anchor=(1.17, 1), fontsize=12)  # Set legend fontsize\n",
    "\n",
    "    # Adjust layout and display the plot\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {}\n",
    "for model_name, model in model_dict.items():\n",
    "    print(f'-- Evaluating {model_name} --')\n",
    "    results[model_name] = {}\n",
    "    competency_analyser.evaluate_competency(model=model)\n",
    "    results[model_name]['scores'] = [{'F1': v['F1'], 'N1': v['N1'], 'score': v['score']} for v in competency_analyser.datasets.values()]\n",
    "    \n",
    "    # Extract F1, N1, and scores from the datasets\n",
    "    f1_scores = [v['F1'] for v in competency_analyser.datasets.values()]\n",
    "    n1_scores = [v['N1'] for v in competency_analyser.datasets.values()]\n",
    "    scores = [v['score'] for v in competency_analyser.datasets.values()]\n",
    "\n",
    "    f1_scores_ = []\n",
    "    n1_scores_ = []\n",
    "    scores_ = []\n",
    "    for f1, n1, score in zip(f1_scores, n1_scores, scores):\n",
    "        if not (np.isnan(f1) or np.isnan(n1) or np.isnan(score)).any():\n",
    "            f1_scores_.append(f1)\n",
    "            n1_scores_.append(n1)\n",
    "            scores_.append(score)\n",
    "\n",
    "    # Set up a grid for the heatmap\n",
    "    f1_range = np.linspace(min(f1_scores), max(f1_scores), 100)\n",
    "    n1_range = np.linspace(min(n1_scores), max(n1_scores), 100)\n",
    "    f1_mesh, n1_mesh = np.meshgrid(f1_range, n1_range)\n",
    "\n",
    "    # Use k-nearest neighbors regression to predict accuracy at each grid point\n",
    "    knn_regressor = KNeighborsRegressor(n_neighbors=20, weights='distance')\n",
    "    knn_regressor.fit(np.column_stack((f1_scores_, n1_scores_)), scores_)\n",
    "\n",
    "    # Predict the score at each grid point\n",
    "    score_grid = knn_regressor.predict(np.column_stack((f1_mesh.ravel(), n1_mesh.ravel())))\n",
    "\n",
    "    # Reshape the predicted score to the shape of the meshgrid\n",
    "    score_grid = score_grid.reshape(f1_mesh.shape)\n",
    "    results[model_name]['score_grid'] = score_grid\n",
    "\n",
    "grid = np.zeros((100,100,10))\n",
    "for i, k in enumerate(results.keys()):\n",
    "    grid[:,:,i] = results[k]['score_grid']\n",
    "\n",
    "compare_models(grid, f1_mesh, n1_mesh)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
