{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "import mrmr\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the list of feature names\n",
    "feature_names =  ['age', 'rank', 'height', 'weight', 'bmi', 'bp_s', 'bp_d', 'bp', 'map', 'smoker', 'alcohol', 'faam', 'eilp', 'chronicity', 'ttp', 'ttd', 'ttt', 'wait time', 'time_dg', 'co_morb', 'prior_injuries', 'prior_surgery', 'prior_courses']\n",
    "\n",
    "# Read the feature data from a CSV file into a Pandas DataFrame\n",
    "X = pd.read_csv('data//X.csv', header=None)\n",
    "\n",
    "# Assign the feature names to the columns of the DataFrame\n",
    "X.columns = feature_names\n",
    "\n",
    "# Read the target variable (labels) from a CSV file into a Pandas DataFrame\n",
    "y = pd.read_csv('data//y.csv', header=None)\n",
    "\n",
    "# Assign a column name ('labels') to the target variable\n",
    "y.columns = ['labels']\n",
    "\n",
    "# Use the mrmr_classif function to select features based on minimum redundancy maximum relevance\n",
    "selected_features = mrmr.mrmr_classif(X, y.iloc[:, 0].values, K=X.shape[1])\n",
    "\n",
    "# Initialize a list with the first selected feature\n",
    "valid_features = [selected_features[0]]\n",
    "\n",
    "# Iterate over the remaining selected features and check for correlation with already selected features\n",
    "for feat in range(1, X.shape[1]):\n",
    "    # Check if the absolute correlation between the current feature and the valid features is greater than 0.5\n",
    "    if any(abs(X[valid_features + [selected_features[feat]]].corr().iloc[:-1, -1].values) > 0.5):\n",
    "        # If the correlation is greater than 0.5, skip adding the feature to the valid features list\n",
    "        pass\n",
    "    else:\n",
    "        # If the correlation is not greater than 0.5, add the feature to the valid features list\n",
    "        valid_features.append(selected_features[feat])\n",
    "\n",
    "# Create a new DataFrame (X_) containing only the selected valid features\n",
    "X_ = X[valid_features].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.pipeline import Pipeline\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import cross_validate, StratifiedKFold, RandomizedSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid_logistic = {\n",
    "    'C': [0.001, 0.01, 0.1, 0.5, 1, 10, 100],\n",
    "    'penalty': ['l1', 'l2'],\n",
    "    'solver': ['liblinear', 'saga']\n",
    "}\n",
    "\n",
    "param_grid_svc = {\n",
    "    'C': [0.001, 0.01, 0.1, 0.5, 1, 10, 100],\n",
    "    'kernel': ['linear', 'rbf', 'poly', 'sigmoid'],\n",
    "    'gamma': ['scale', 'auto']\n",
    "}\n",
    "\n",
    "param_grid_rf = {\n",
    "    'n_estimators': [10, 20, 50, 100],\n",
    "    'max_features': ['sqrt', 'log2'],\n",
    "    'max_depth': [1, 2]\n",
    "}\n",
    "\n",
    "param_grid_lda = {}  # LinearDiscriminantAnalysis does not have hyperparameters to tune\n",
    "\n",
    "param_grid_xgb = {\n",
    "    'learning_rate': [0.001, 0.005, 0.01, 0.05, 0.1],\n",
    "    'max_depth': [1, 2],\n",
    "    'n_estimators': [10, 20, 50, 100],\n",
    "    'booster': ['gbtree', 'gblinear', 'dart']\n",
    "}\n",
    "\n",
    "models = [LogisticRegression(class_weight='balanced'),\n",
    "          SVC(probability=True, max_iter=100,class_weight='balanced'),\n",
    "          RandomForestClassifier(class_weight='balanced',warm_start=True),\n",
    "          Pipeline([('smote', SMOTE(sampling_strategy='auto', random_state=42)), ('clf', LinearDiscriminantAnalysis())]),\n",
    "          Pipeline([('smote', SMOTE(sampling_strategy='auto', random_state=42)), ('clf', XGBClassifier())])]\n",
    "\n",
    "model_names = ['Logistic Regression',\n",
    "               'SVM',\n",
    "               'Random Forest',\n",
    "               'Linear Discriminant Analysis',\n",
    "               'XGBoost']\n",
    "\n",
    "# Perform hyperparameter tuning for each model\n",
    "tuned_models = []\n",
    "for model, param_grid in zip(models, [param_grid_logistic, param_grid_svc, param_grid_rf, param_grid_lda, param_grid_xgb]):\n",
    "    if isinstance(model, Pipeline):\n",
    "        # If the model is a pipeline, update the 'clf' step with RandomizedSearchCV\n",
    "        model_name = model.steps[-1][0]\n",
    "        tuned_model = Pipeline([\n",
    "            ('smote', model.steps[0][1]),  # Assuming the first step is 'smote'\n",
    "            ('clf', RandomizedSearchCV(model.steps[-1][1], param_distributions=param_grid, scoring='roc_auc', n_iter=10, random_state=42, cv=4))\n",
    "        ])\n",
    "    else:\n",
    "        # If the model is not a pipeline, wrap it with RandomizedSearchCV\n",
    "        model_name = model.__class__.__name__\n",
    "        tuned_model = RandomizedSearchCV(model, param_distributions=param_grid, scoring='roc_auc', n_iter=10, random_state=42, cv=4)\n",
    "\n",
    "    tuned_models.append(tuned_model)\n",
    "\n",
    "# Create the model dictionary\n",
    "model_dict = dict(zip(model_names, tuned_models))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sensitivity_function(y_true, y_pred):\n",
    "  conf_matrix = metrics.confusion_matrix(y_true, y_pred)\n",
    "  TP = conf_matrix[1][1]\n",
    "  TN = conf_matrix[0][0]\n",
    "  FP = conf_matrix[0][1]\n",
    "  FN = conf_matrix[1][0]\n",
    "\n",
    "  # calculate the sensitivity\n",
    "  conf_sensitivity = (TP / float(TP + FN))\n",
    "  return conf_sensitivity\n",
    "\n",
    "def specificity_function(y_true, y_pred):\n",
    "  conf_matrix = metrics.confusion_matrix(y_true, y_pred)\n",
    "  TP = conf_matrix[1][1]\n",
    "  TN = conf_matrix[0][0]\n",
    "  FP = conf_matrix[0][1]\n",
    "  FN = conf_matrix[1][0]\n",
    "\n",
    "  # calculate the specificity\n",
    "  conf_specificity = (TN / float(TN + FP))\n",
    "  return conf_specificity\n",
    "\n",
    "def orp_fpr_function(y_true, y_pred):\n",
    "  fpr, tpr, thresholds = metrics.roc_curve(y_true, y_pred)\n",
    "  i = np.arange(len(tpr))\n",
    "  roc = pd.DataFrame({'fpr' : pd.Series(fpr, index=i),'tpr' : pd.Series(tpr, index = i), '1-fpr' : pd.Series(1-fpr, index = i), 'tf' : pd.Series(tpr - (1-fpr), index = i), 'thresholds' : pd.Series(thresholds, index = i)})\n",
    "  roc_t = roc.iloc[(roc.tf-0).abs().argsort()[:1]]\n",
    "  return list(roc_t['fpr'])[0]\n",
    "\n",
    "def orp_tpr_function(y_true, y_pred):\n",
    "  fpr, tpr, thresholds = metrics.roc_curve(y_true, y_pred)\n",
    "  i = np.arange(len(tpr))\n",
    "  roc = pd.DataFrame({'fpr' : pd.Series(fpr, index=i),'tpr' : pd.Series(tpr, index = i), '1-fpr' : pd.Series(1-fpr, index = i), 'tf' : pd.Series(tpr - (1-fpr), index = i), 'thresholds' : pd.Series(thresholds, index = i)})\n",
    "  roc_t = roc.iloc[(roc.tf-0).abs().argsort()[:1]]\n",
    "  return list(roc_t['tpr'])[0]\n",
    "\n",
    "def evaluate(y_test, y_hat, y_prob):\n",
    "  acc = metrics.accuracy_score(y_test, y_hat)\n",
    "  sens = sensitivity_function(y_test, y_hat)\n",
    "  spec = specificity_function(y_test, y_hat)\n",
    "  auc = metrics.roc_auc_score(y_test, y_prob)\n",
    "  orp_fpr = orp_fpr_function(y_test, y_prob)\n",
    "  orp_tpr = orp_tpr_function(y_test, y_prob)\n",
    "  return [acc, sens, spec, auc, orp_fpr, orp_tpr]\n",
    "\n",
    "sensitivity = metrics.make_scorer(sensitivity_function, greater_is_better=True)\n",
    "specificity = metrics.make_scorer(specificity_function, greater_is_better=True)\n",
    "orp_fpr = metrics.make_scorer(orp_fpr_function, greater_is_better=True, needs_proba=True)\n",
    "orp_tpr = metrics.make_scorer(orp_tpr_function, greater_is_better=True, needs_proba=True)\n",
    "scoring = {'Accuracy' : 'accuracy', 'Sensitivity': sensitivity, 'Specificity' : specificity, 'AUC': 'roc_auc', 'ORP FPR' : orp_fpr, 'ORP TPR' : orp_tpr, 'F1': 'f1'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "for name, model in model_dict.items():\n",
    "    scores = cross_validate(estimator=model, X=X_.values, y=y['labels'].values, cv=StratifiedKFold(n_splits=5,shuffle=True,random_state=42),scoring=scoring,return_train_score=True)\n",
    "    results.append([name,\n",
    "                    str(np.round(np.mean(scores['train_Accuracy']),2)) + ' ± ' + str(np.round(np.std(scores['train_Accuracy']),2)), \n",
    "                    str(np.round(np.mean(scores['train_Sensitivity']),2)) + ' ± ' + str(np.round(np.std(scores['train_Sensitivity']),2)), \n",
    "                    str(np.round(np.mean(scores['train_Specificity']),2)) + ' ± ' + str(np.round(np.std(scores['train_Specificity']),2)), \n",
    "                    str(np.round(np.mean(scores['train_AUC']),2)) + ' ± ' + str(np.round(np.std(scores['train_AUC']),2)), \n",
    "                    str(np.round(np.mean(scores['train_ORP FPR']),2)) + ' ± ' + str(np.round(np.std(scores['train_ORP FPR']),2)), \n",
    "                    str(np.round(np.mean(scores['train_ORP TPR']),2)) + ' ± ' + str(np.round(np.std(scores['train_ORP TPR']),2))])\n",
    "    results.append([name,\n",
    "                    str(np.round(np.mean(scores['test_Accuracy']),2)) + ' ± ' + str(np.round(np.std(scores['test_Accuracy']),2)), \n",
    "                    str(np.round(np.mean(scores['test_Sensitivity']),2)) + ' ± ' + str(np.round(np.std(scores['test_Sensitivity']),2)), \n",
    "                    str(np.round(np.mean(scores['test_Specificity']),2)) + ' ± ' + str(np.round(np.std(scores['test_Specificity']),2)), \n",
    "                    str(np.round(np.mean(scores['test_AUC']),2)) + ' ± ' + str(np.round(np.std(scores['test_AUC']),2)), \n",
    "                    str(np.round(np.mean(scores['test_ORP FPR']),2)) + ' ± ' + str(np.round(np.std(scores['test_ORP FPR']),2)), \n",
    "                    str(np.round(np.mean(scores['test_ORP TPR']),2)) + ' ± ' + str(np.round(np.std(scores['test_ORP TPR']),2))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = pd.DataFrame(data = np.vstack(results), columns = ['Model', 'Accuracy', 'Sensitivity', 'Specificity', 'AUROC', 'ORP FPR', 'ORP TPR'])\n",
    "results_df.to_csv('data//classification_result.csv', index=None)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
